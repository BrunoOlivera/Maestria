{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYySPW4nFUC5"
   },
   "source": [
    "# Machine Learning con atributos para clasificación de tweets por polaridad\n",
    "\n",
    "En este laboratorio se trabajará con un corpus creado a partir de los corpus del TASS (2017 y 2018) (http://www.sepln.org/workshops/tass/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HppkxukK0d4B"
   },
   "outputs": [],
   "source": [
    "# Ejecutar este fragmento si se ejecuta en colab\n",
    "# Debes autorizar a colab a acceder a tus archivos\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/lab')\n",
    "\n",
    "# En esta variable va la carpeta donde están los laboratorios\n",
    "carpeta_laboratorios = 'pln/AnaTex/lab aiala'\n",
    "\n",
    "path_laboratorios = '/lab/My Drive/' + carpeta_laboratorios + '/'\n",
    "print('Los archivos en tu carpeta de laboratoros son:')\n",
    "os.listdir(path_laboratorios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOhveGs5CTW5"
   },
   "source": [
    "# Carga de la información de los tweets\n",
    "\n",
    "Complete el siguiente código de modo de cargar un corpus (archivo csv) y retornar un vector con los tweets y un vector con las polaridades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dVXNwLxf8hkJ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "        \n",
    "def load_corpus(corpusFile):\n",
    "    with open(corpusFile, encoding='utf8', newline='') as csvfile:\n",
    "        tweets = list(csv.reader(csvfile, delimiter=','))\n",
    "    polarities = [] # polaridades\n",
    "    originalTweets = [] # tweets\n",
    "    for t in tweets:\n",
    "        polarities += [t[2]]\n",
    "        originalTweets += [t[1]]\n",
    "\n",
    "    return np.asarray(originalTweets), np.asarray(polarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OttWkk2uDO2X"
   },
   "source": [
    "\n",
    "# Bag of Words (BOW)\n",
    "\n",
    "A continuación se generarán features de tipo Bag of Words para los primeros experimentos.\n",
    "\n",
    "Para esto utilice la clase CountVectorizer de sklearn, use fit_transform y transform (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HRcrmO-f0LKG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impresionante las vistas por dentro del nicho del buda ... y mucho vértigo  #Afganistán #Bamiyán http://t.co/tFB8iknI\n",
      "P\n",
      "(8315, 24004)\n",
      "(1133, 24004)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cargar info de los corpus train (para entrenar) y devel (para evaluar experimentos) \n",
    "# origTrain, polTrain = load_corpus(path_laboratorios + 'train.csv')\n",
    "# origDevel, polDevel = load_corpus(path_laboratorios + 'devel.csv')\n",
    "origTrain, polTrain = load_corpus('train.csv')\n",
    "origDevel, polDevel = load_corpus('devel.csv')\n",
    "\n",
    "# imprima algunos tweets y su polaridad para verificar que la carga del corpus es correcta\n",
    "\n",
    "print(origTrain[0])\n",
    "print(polTrain[0])\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# genere las features de entrenamiento con fit_transform\n",
    "train_features = vectorizer.fit_transform(origTrain)\n",
    "\n",
    "# genere las features para el corpus 'devel' con transform\n",
    "devel_features = vectorizer.transform(origDevel)\n",
    "\n",
    "# imprima los tamaños de las matrices generadas \n",
    "print(np.shape(train_features))\n",
    "print(np.shape(devel_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxFJ0KGMI7MP"
   },
   "source": [
    "Saltee esta sección para probar los clasificadores con las features ya generadas (última sección del notebook).\n",
    "\n",
    "Luego de hacer esos experimentos vuelva a esta sección y genere las features nuevamente, siguiendo las instrucciones siguientes:\n",
    "\n",
    "Pruebe con diferentes cantidades de palabras como features usando la clase SelectKBest (10, 20, 50, 100, 200, 1000).\n",
    "\n",
    "Imprima las palabras que selecciona SelectKBest (hasta 100).\n",
    "\n",
    "Vuelva a probar los clasificadores de la sección siguiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Y4syCevBJFcN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['co' 'gracias' 'http' 'no' 'peor' 'pero' 'portada' 'que' 'se' 'veo']\n"
     ]
    }
   ],
   "source": [
    "kbest = {}\n",
    "vectorizerKBest = {}\n",
    "for i in [10, 20, 50, 100, 200, 1000]:\n",
    "    kbest[i] = SelectKBest(k=i)\n",
    "    vectorizerKBest[i] = make_pipeline(vectorizer, kbest[i])\n",
    "\n",
    "    ## genere las features usando vectorizerKBest en vez de vectorizer\n",
    "    feats = vectorizerKBest[i].fit_transform(origDevel, polDevel)\n",
    "\n",
    "\n",
    "    ## imprima las k palabras seleccionadas por SelectKBest\n",
    "    print('==>i: ',i,np.asarray(vectorizer.get_feature_names())[kbest[i].get_support()])\n",
    "\n",
    "\n",
    "# kbest = SelectKBest(k=10)\n",
    "# vectorizerKBest = make_pipeline(vectorizer, kbest)\n",
    "\n",
    "# ## genere las features usando vectorizerKBest en vez de vectorizer\n",
    "# feats = vectorizerKBest.fit_transform(origDevel, polDevel)\n",
    "\n",
    "\n",
    "# ## imprima las k palabras seleccionadas por SelectKBest\n",
    "# print(np.asarray(vectorizer.get_feature_names())[kbest.get_support()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssAIxwl9PONr"
   },
   "source": [
    "# Léxico subjetivo\n",
    "\n",
    "Vamos a incluir features basadas en un léxico subjetivo, además de las features basadas en BOW.\n",
    "\n",
    "El léxico contiene todas las formas correspondientes a cada lema. Está organizado en dos archivos csv, uno con palabras positivas y otro con palabras negativas.\n",
    "\n",
    "Se provee una función para cargar el léxico.\n",
    "\n",
    "Luego volvemos a probar los dos clasificadores de la última sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "pjYx_KGHPoju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8315, 2)\n",
      "(1133, 2)\n",
      "Impresionante las vistas por dentro del nicho del buda ... y mucho vértigo  #Afganistán #Bamiyán http://t.co/tFB8iknI\n",
      "[ 1.  0.]\n",
      "Al 90% en Asturias: PSOE: 16; FAC: 13; PP: 10: IU: 5; UPyD:1.\n",
      "[ 0.  0.]\n",
      "+1 RT @nuriarocagranel: Porqué utiliza Chacón ese tono????? A mí, la verdad, es que esa forma de hablar me parece bastante artificial\n",
      "[ 1.  1.]\n",
      "Estos días trataré de hacer deporte con mis amigos y desconectar.\n",
      "[ 1.  0.]\n",
      "(8315, 24006)\n",
      "(1133, 24006)\n",
      "(8315, 2)\n",
      "(1133, 2)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def read_lexicon(lex_file):\n",
    "    # cargo lexico\n",
    "#     with open(path_laboratorios + lex_file, encoding='utf8') as lex:\n",
    "    with open(lex_file, encoding='utf8') as lex:\n",
    "        lex_list = []\n",
    "        for word in lex:\n",
    "            lex_list.append(word.rstrip('\\r\\n').lower())\n",
    "    return lex_list\n",
    "\n",
    "def getLexFeats(tweets, posLex, negLex):\n",
    "    # calculo cantidad de lemas positivos y negativos en cada tweet del corpus\n",
    "    res = np.zeros((np.shape(tweets)[0],2))\n",
    "    print(np.shape(res))\n",
    "    ### completar\n",
    "    for i,t in enumerate(tweets):\n",
    "        for w in t.split():\n",
    "            if w in posLex:\n",
    "                res[i,0] += 1\n",
    "            if w in negLex:\n",
    "                res[i,1] += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "# construyo features basadas en léxicos para train y devel\n",
    "# cargo léxicos\n",
    "# posLex = set(read_lexicon('lexico_pos_lemas_grande.csv'))\n",
    "# negLex = set(read_lexicon('lexico_neg_lemas_grande.csv'))\n",
    "\n",
    "posLex = set(read_lexicon('lexico_pos_palabras_grande.csv'))\n",
    "negLex = set(read_lexicon('lexico_neg_palabras_grande.csv'))\n",
    "\n",
    "# obtengo features para train y devel\n",
    "lexFeatsTrain = getLexFeats(origTrain, posLex, negLex)\n",
    "lexFeatsDevel = getLexFeats(origDevel, posLex, negLex)\n",
    "\n",
    "# verifico los valores obtenidos\n",
    "print(origTrain[0])\n",
    "print(lexFeatsTrain[0])\n",
    "print(origTrain[11])\n",
    "print(lexFeatsTrain[11])\n",
    "print(origTrain[200])\n",
    "print(lexFeatsTrain[200])\n",
    "print(origDevel[0])\n",
    "print(lexFeatsDevel[0])\n",
    "\n",
    "# print('=>',np.shape(origTrain))\n",
    "# print('=>',np.shape(train_features))\n",
    "# print('=>',np.shape(lexFeatsTrain))\n",
    "\n",
    "# print(train_features[0,:])\n",
    "# print(np.shape(train_features[0,:]))\n",
    "# print(lexFeatsTrain[0])\n",
    "\n",
    "# combino features BOW con features basadas en léxico\n",
    "# train_x = np.hstack((train_features, lexFeatsTrain))\n",
    "# devel_x = np.hstack((devel_features, lexFeatsDevel))\n",
    "train_x = scipy.sparse.hstack((train_features, lexFeatsTrain))\n",
    "devel_x = scipy.sparse.hstack((devel_features, lexFeatsDevel))\n",
    "\n",
    "# verifico dimensiones\n",
    "print(train_x.shape)\n",
    "print(devel_x.shape)\n",
    "print(lexFeatsTrain.shape)\n",
    "print(lexFeatsDevel.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbBcTeYbNSQ7"
   },
   "source": [
    "# Clasificadores\n",
    "\n",
    "Entrene dos clasificadores: Multinomial Naive Bayes y SVM para probar las diferentes configuraciones de features que desee.\n",
    "\n",
    "Imprima accuracy de cada clasificador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "z7JztfV294NM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy:  0.554280670786\n",
      "NB Accuracy (+lex):  0.573698146514\n",
      "SVM Accuracy:  0.353045013239\n",
      "SVM Accuracy (+lex):  0.374227714034\n"
     ]
    }
   ],
   "source": [
    "# clasificadores\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "clf_nb.fit(train_features, polTrain)\n",
    "print(\"NB Accuracy: \", clf_nb.score(devel_features, polDevel))\n",
    "\n",
    "clf_nb_lex = MultinomialNB()\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "clf_nb_lex.fit(train_x, polTrain)\n",
    "print(\"NB Accuracy (+lex): \", clf_nb_lex.score(devel_x, polDevel))\n",
    "\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svm = SVC()\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "clf_svm.fit(train_features, polTrain)\n",
    "print(\"SVM Accuracy: \", clf_svm.score(devel_features, polDevel))\n",
    "\n",
    "clf_svm_lex = SVC()\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "clf_svm_lex.fit(train_x, polTrain)\n",
    "print(\"SVM Accuracy (+lex): \", clf_svm_lex.score(devel_x, polDevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy(feats):  0.463371579876\n",
      "SVM Accuracy(feats):  0.470432480141\n"
     ]
    }
   ],
   "source": [
    "clf_nb.fit(feats, polDevel)\n",
    "print(\"NB Accuracy(feats): \", clf_nb.score(feats, polDevel))\n",
    "clf_svm.fit(feats, polDevel)\n",
    "print(\"SVM Accuracy(feats): \", clf_svm.score(feats, polDevel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVjHkT-TNmru"
   },
   "source": [
    "# Preguntas finales\n",
    "\n",
    "1. ¿Qué features son mejores para cada clasificador?\n",
    "\n",
    "2. ¿Qué puede decir sobre las palabras seleccionadas por SelectKBest?\n",
    "\n",
    "3. Calcule precision, recall y F1 para cada clase, analice para qué clase se obtienen los mejores/peores resultados.\n",
    "\n",
    "4. Elija los mejores atributos para cada clasificador y evalúe sobre el corpus 'test'.\n",
    "\n",
    "5. Opcional: Pruebe otros clasificadores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oAtjM0GA7QJ-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "tweetsML_BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
