{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYySPW4nFUC5"
   },
   "source": [
    "# Machine Learning con atributos para clasificación de tweets por polaridad\n",
    "\n",
    "En este laboratorio se trabajará con un corpus creado a partir de los corpus del TASS (2017 y 2018) (http://www.sepln.org/workshops/tass/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HppkxukK0d4B"
   },
   "outputs": [],
   "source": [
    "# Ejecutar este fragmento si se ejecuta en colab\n",
    "# Debes autorizar a colab a acceder a tus archivos\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/lab')\n",
    "\n",
    "# En esta variable va la carpeta donde están los laboratorios\n",
    "carpeta_laboratorios = ''\n",
    "\n",
    "path_laboratorios = '/lab/My Drive/' + carpeta_laboratorios + '/'\n",
    "print('Los archivos en tu carpeta de laboratoros son:')\n",
    "os.listdir(path_laboratorios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOhveGs5CTW5"
   },
   "source": [
    "# Carga de la información de los tweets\n",
    "\n",
    "Complete el siguiente código de modo de cargar un corpus (archivo csv) y retornar un vector con los tweets y un vector con las polaridades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dVXNwLxf8hkJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "        \n",
    "def load_corpus(corpusFile):\n",
    "    with open(corpusFile, encoding='utf8', newline='') as csvfile:\n",
    "        tweets = list(csv.reader(csvfile, delimiter=','))\n",
    "    polarities = [] # polaridades\n",
    "    originalTweets = [] # tweets\n",
    "    \n",
    "    ### completar\n",
    "\n",
    "    return np.asarray(originalTweets), np.asarray(polarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OttWkk2uDO2X"
   },
   "source": [
    "\n",
    "# Word embeddings\n",
    "\n",
    "A continuación se generarán features a partir de un modelo de word embeddings.\n",
    "\n",
    "Para esto debe tener en su directorio de trabajo los archivos emb39word2vec.ny y emb39word2vec.txt, que contienen información sobre los vectores asociados a cada palabra. También necesitará el script load_vectors.py que contiene una función para cargar el modelo vectorial.\n",
    "\n",
    "Luego se grafican algunas palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HRcrmO-f0LKG"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-f3e73ff60580>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f3e73ff60580>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    default_vector = np.zeros(len(npy[0]))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Carga el modelo de embeddings desde el archivo emb39-word2vec\n",
    "    \"\"\"\n",
    "#   file_name = path_laboratorios + 'emb39-word2vec'\n",
    "    file_name = 'emb39-word2vec'\n",
    "    fname_txt = file_name + '.txt'\n",
    "    fname_npy = file_name + '.npy'\n",
    "\n",
    "    # load npy and txt\n",
    "    npy = np.load(fname_npy)\n",
    "    txt = open(fname_txt, encoding='utf-8').read().splitlines()  \n",
    "\n",
    "    # index str\n",
    "    w2ind = dict()\n",
    "    for ind, wd in enumerate(txt):\n",
    "        w2ind[wd] = ind\n",
    "\n",
    "    # w2v function\n",
    "    def w2v(wd):\n",
    "        default_vector = np.zeros(len(npy[0]))\n",
    "        try:\n",
    "            return npy[w2ind[wd]]\n",
    "        except KeyError:\n",
    "        #      print (wd + ' not found in vecset')\n",
    "          return default_vector\n",
    "\n",
    "    wd2vect = w2v\n",
    "    wd2vect.name = file_name\n",
    "    wd2vect.dim = len(npy[0])\n",
    "    return wd2vect\n",
    "\n",
    "# cargo el modelo vectorial para usar en diferentes secciones del notebook\n",
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grafica de palabras relacionadas con representación de embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "palabras = ['mujer', 'mujeres', 'niña', 'hombre', 'hombres', 'niño', 'perro', 'gato']\n",
    "representacion = np.array([model(palabra) for palabra in palabras])\n",
    "\n",
    "print(representacion.shape)\n",
    "pca = PCA(n_components=2)\n",
    "representacion_simplificada = pca.fit_transform(representacion)\n",
    "\n",
    "plt.scatter(representacion_simplificada[:, 0], representacion_simplificada[:, 1])\n",
    "for i, palabra in enumerate(palabras):\n",
    "  plt.annotate(palabra, xy=representacion_simplificada[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxFJ0KGMI7MP"
   },
   "source": [
    "# Vector promedio de un texto\n",
    "\n",
    "A continuación se define una función para calcular el vector promedio de cada tweet, que luego se utilizará para generar las features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y4syCevBJFcN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mean_vector_text(text):\n",
    "    \n",
    "    vectors = [model(word) for word in text if model(word).any()]\n",
    "    meanvec = (sum(vectors) / len(vectors)) if vectors else np.zeros(300)\n",
    "\n",
    "    return meanvec\n",
    "\n",
    "\n",
    "# pruebe la función mean_vector_text con los siguientes textos: \n",
    "# ['estoy', 'en', 'la', 'facultad', 'trabajando']\n",
    "# ['facxladtx']\n",
    "\n",
    "### completar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de features \n",
    "\n",
    "En este bloque se cargan los corpus de tweets y se calcula el vector promedio de cada tweet.\n",
    "\n",
    "Los 300 valores reales correspondientes al vector promedio de un twwet serán las features para entrenamiento.\n",
    "\n",
    "Luego de generar las features, pruebe los clasificadores de la última sección del notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# cargar info de los corpus train (para entrenar) y devel (para evaluar experimentos) \n",
    "origTrain, polTrain = load_corpus(path_laboratorios + 'train3000.csv')\n",
    "origDevel, polDevel = load_corpus(path_laboratorios + 'devel.csv')\n",
    "\n",
    "# imprima algunos tweets y su polaridad para verificar que la carga del corpus sea correcta\n",
    " \n",
    "### completar\n",
    "\n",
    "\n",
    "# generación de features: \n",
    "# para cada tweet se van a generar 300 features, \n",
    "# que son los 300 valores reales del vector promedio \n",
    "# de los vectores de todas las palabras del tweet\n",
    "\n",
    "# completar función para calcular el mean vector de cada tweet de un corpus\n",
    "def tweetsMeanVec(tweets):\n",
    "\n",
    "    ### completar\n",
    "        \n",
    "    return np.asarray(twsMV)\n",
    "        \n",
    "    \n",
    "# genere las features para los tweets del corpus 'train'\n",
    "\n",
    "trainMV = ### completar\n",
    "\n",
    "\n",
    "# genere las features para los tweets del corpus 'devel'\n",
    "\n",
    "develMV = ### completar\n",
    "\n",
    "\n",
    "# imprima los tamaños de las matrices generadas \n",
    "\n",
    "### completar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssAIxwl9PONr"
   },
   "source": [
    "# Léxico subjetivo\n",
    "\n",
    "En esta sección vamos a incluir features basadas en un léxico subjetivo, además de las features basadas en el vector promedio del tweet.\n",
    "\n",
    "El léxico contiene todas las formas correspondientes a cada lema. Está organizado en dos archivos csv, uno con palabras positivas y otro con palabras negativas.\n",
    "\n",
    "Se provee una función para cargar el léxico.\n",
    "\n",
    "Piense de qué forma usar el léxico para generar features, aplicando alguna de las ideas siguientes (u otras): contar cantidad de palabras incluidas en los léxicos, calcular distancias de las palabras del tweet a las palabras de los léxicos (usando los embeddings) o al vector promedio de cada léxico, calcular distancia del vector promedio del tweet a las palabras del léxico, etc. \n",
    "\n",
    "\n",
    "Luego vuelva a probar los dos clasificadores de la última sección.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pjYx_KGHPoju"
   },
   "outputs": [],
   "source": [
    "def read_lexicon(lex_file):\n",
    "    # cargo lexico\n",
    "    with open(lex_file, encoding='utf8') as lex:\n",
    "        lex_list = []\n",
    "        for word in lex:\n",
    "            lex_list.append(word.rstrip('\\r\\n').lower())\n",
    "    return lex_list\n",
    "\n",
    "def getLexFeats(tweets):\n",
    "    # aplique alguna idea de las sugeridas u otras que usted defina para calcular features basadas en léxicos\n",
    "    \n",
    "    ### completar\n",
    "    \n",
    "\n",
    "\n",
    "# construyo features basadas en léxicos para train y devel\n",
    "# cargo léxicos\n",
    "posLex = set(read_lexicon(path_laboratorios + 'lexico_pos_palabras_grande.csv'))\n",
    "negLex = set(read_lexicon(path_laboratorios + 'lexico_neg_palabras_grande.csv'))\n",
    "\n",
    "# obtengo features para train y devel\n",
    "lexFeatsTrain = ### completar\n",
    "lexFeatsDevel = ### completar\n",
    "\n",
    "\n",
    "# verifico los valores obtenidos\n",
    "\n",
    "\n",
    "# combino features obtenidas a partir del vector promedio con features basadas en léxico\n",
    "trainMVLex = np.hstack((### completar))\n",
    "develMVLex = np.hstack((### completar))\n",
    "\n",
    "# verifico dimensiones\n",
    "print(trainMVLex.shape)\n",
    "print(develMVLex.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbBcTeYbNSQ7"
   },
   "source": [
    "# Clasificadores\n",
    "\n",
    "Entrene dos clasificadores: Multilayer Perceptron y SVM para probar las diferentes configuraciones de features que desee.\n",
    "\n",
    "Imprima accuracy de cada clasificador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z7JztfV294NM"
   },
   "outputs": [],
   "source": [
    "# clasificadores\n",
    "\n",
    "# Normalización de atributos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(trainMV)\n",
    "X_devel = scaler.transform(develMV)\n",
    "\n",
    "# Para un segundo experimento usando el léxico, utilice estas features\n",
    "#X_train = scaler.fit_transform(trainMVLex)\n",
    "#X_devel = scaler.transform(develMVLex)\n",
    "\n",
    "\n",
    "# Multi Layer Perceptron (MLP)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_mlp = MLPClassifier(activation='logistic', hidden_layer_sizes=(150,100,50), max_iter=500)\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "### completar\n",
    "\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svm = SVC()\n",
    "## utilice fit para entrenar y score para obtener la accuracy\n",
    "### completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVjHkT-TNmru"
   },
   "source": [
    "# Preguntas finales\n",
    "\n",
    "1. ¿Qué features son mejores para cada clasificador?\n",
    "\n",
    "2. Calcule precision, recall y F1 para cada clase, analice para qué clase se obtienen los mejores/peores resultados.\n",
    "\n",
    "3. Elija los mejores atributos para cada clasificador y evalúe sobre el corpus 'test'.\n",
    "\n",
    "4. Opcional: Pruebe otras combinaciones de features y otros clasificadores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tweetsML_BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
