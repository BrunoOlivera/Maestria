col=c("#00AFBB", "#6BB82E", "#B30417", "#325b82"), lty=c(1,1), cex=0.9,
title="Comparación ROC Modelos", text.font=4, bg='lightblue')
# Ejercicio 2
set.seed(123)
dlearn=read.csv('breast-cancer-wisconsin.data',sep=',',header = FALSE)
dlearn[dlearn == '?'] = as.numeric(NA)
dlearn <- na.omit(dlearn)
dlearn[,] <- sapply(dlearn[,], as.numeric)
dlearn[1] <- NULL
colnames(dlearn) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','Class')
dlearn[,10] <- as.numeric(dlearn[,10]==4)
smp_size <- floor(0.8 * nrow(dlearn))
train_ind <- sample(seq_len(nrow(dlearn)), size = smp_size)
X_train <- dlearn[train_ind, ]
X_test <- dlearn[-train_ind, ]
y_train <- X_train[,10]
y_test <- X_test[,10]
# a)
glm.modelo=glm(Class~.,family=binomial, data=X_train, maxit=100)
summary(glm.modelo)
# b)
chi2=glm.modelo$null.deviance - glm.modelo$deviance
ddl=glm.modelo$df.null-glm.modelo$df.residual
pvalor=pchisq(chi2,ddl,lower.tail=F)
pvalor
anova(glm.modelo)
# c)
res <- NULL
for(var in row.names(summary(glm.modelo)$coefficients)) {
if(var != '(Intercept)'){
if(summary(glm.modelo)$coefficients[var,4] < .05){
res <- rbind(res,var)
}
}
}
formula <- as.formula(paste("Class~", paste(res, collapse="+")))
formula
reduced.model <- glm(formula,family=binomial, data=X_train, maxit=100)
summary(reduced.model)
# d)
empty.model <- glm(Class~1,family = binomial, data=X_train)
forward.model <- stepAIC(empty.model, scope=formula(glm.modelo), direction="forward", trace = FALSE)
summary(forward.model)
# e)
stepwise.model <- stepAIC(glm.modelo, direction = "both", trace = FALSE)
summary(stepwise.model)
# f)
AIC(forward.model)-AIC(stepwise.model)
AIC(reduced.model)-AIC(stepwise.model)
# g)
forward.model.preds <- as.numeric(predict(forward.model, X_test[,-10], type='response') > .5)
table(forward.model.preds,y_test)
mean(forward.model.preds==y_test)
stepwise.model.preds <- as.numeric(predict(stepwise.model, X_test[,-10], type='response') > .5)
table(stepwise.model.preds,y_test)
mean(stepwise.model.preds==y_test)
reduced.model.preds <- as.numeric(predict(reduced.model, X_test[,-10], type='response') > .5)
table(reduced.model.preds,y_test)
mean(reduced.model.preds==y_test)
# f)
# Curvas ROC
yhat_modelo=predict(glm.modelo,X_test[,-10],type='response')
yhat_reducido=predict(reduced.model,X_test[,-10],type='response')
yhat_forw=predict(forward.model,X_test[,-10],type='response')
yhat_both=predict(stepwise.model,X_test[,-10],type='response')
rocplot =function (pred , truth , C,...){
predob = prediction (pred , truth)
perf = performance (predob , "tpr", "fpr")
return(perf)}
AUC_ROC =function (pred , truth , ...){
predob = prediction (pred , truth)
Area = performance (predob , "auc")
return(Area@y.values)}
plot(rocplot(yhat_modelo,y_test),col="#00AFBB",main="Curva ROC")
par(new=TRUE)
plot(rocplot(yhat_reducido,y_test),col="#6BB82E",main="Curva ROC")
par(new=TRUE)
plot(rocplot(yhat_forw,y_test),col="#B30417",main="Curva ROC")
par(new=TRUE)
plot(rocplot(yhat_both,y_test),col="#325b82",main="Curva ROC")
par(new=TRUE)
lines(c(seq(0,1,0.01)), c(seq(0,1,0.01)), col = "#FC4E07", type="l", lty=2)
AUC_ROC_mod = AUC_ROC(yhat_modelo,y_test)
AUC_ROC_res = AUC_ROC(yhat_reducido,y_test)
AUC_ROC_fwd = AUC_ROC(yhat_forw,y_test)
AUC_ROC_bot = AUC_ROC(yhat_both,y_test)
legend(0.5, 0.4, legend=c(paste("M.Completo - AUC = ", AUC_ROC_mod) , paste("M.Reducido - AUC = ", AUC_ROC_res),
paste("M.Forward - AUC = ", AUC_ROC_fwd),paste("M.Both - AUC = ", AUC_ROC_bot)),
col=c("#00AFBB", "#6BB82E", "#B30417", "#325b82"), lty=c(1,1), cex=0.9,
title="Comparación ROC Modelos", text.font=4, bg='lightblue')
# a)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X = c(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X = rbind(X,c(x1,x2,y))
View(X)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X = c(t(x1),t(x2),y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X = rbind(X,c(t(x1),t(x2),y))
View(X)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X = c(t(x1),t(x2),y)
X
X = cbind(x1,x2,y)
View(X)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X = cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X = rbind(X,cbind(x1,x2,y))
X
X[1]
X[,1]
y=rbinom(n,1,y)
y
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1])
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1], pch=20)
modelo=lda(y~X)
plot(modelo)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
group <- NA
group[y==0] = 1
group[y==1] = 2
plot(X[,1],X[,2], col=c("red","blue")[group], pch=20)
group <- NA
group[X[,3]==0] = 1
group[X[,3]==1] = 2
plot(X[,1],X[,2], col=c("red","blue")[group], pch=20)
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1], pch=20)
# a)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X1=cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X2=cbind(x1,x2,y)
X=rbind(X1,X2)
# b)
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1], pch=20)
View(X1)
View(X2)
View(X)
S_1=var(X1[1:50,-3])
S_2=var(X2[1:50,-3])
S_1
S_2
S=1/98*(49*S_1+49*S_2)
S
solve(S)
t(S)
S_1=var(X1[1:50,-3])
S_2=var(X2[1:50,-3])
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
S_1=var(X1[1:50,-3])
S_2=var(X2[1:50,-3])
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X1)-colMeans(X2)
w=S_inv%*%as.matrix(d)
X_1=X1[1:50,-3]
X_2=X2[1:50,-3]
S_1=var(X_1)
S_2=var(X_2)
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
modelo=lda(X_1~X_2)
plot(modelo)
X_1=X1[1:50,-3]
X_2=X2[1:50,-3]
S_1=var(X_1)
S_2=var(X_2)
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
X_1=X1[1:50,-3]
X_2=X2[1:50,-3]
S_1=var(X_1)
S_2=var(X_2)
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
# a)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X1=cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X2=cbind(x1,x2,y)
X=rbind(X1,X2)
# b)
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1], pch=20)
# c)
X_1=X1[1:50,-3]
X_2=X2[1:50,-3]
S_1=var(X_1)
S_2=var(X_2)
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
cls
clc
clear
w
modelo$scaling
modelo$scaling/2
modelo$scaling/w
predict(modelo, test)
predict(modelo, data.frame(test))
predict(modelo, data.frame(test))
test=rbind(X1,X2)
test
predict(modelo, data.frame(test))
preds=predict(modelo, data.frame(test))
preds$class
preds$class==test[,3]
sum(preds$class==test[,3])
# % de bien clasificados
sum(preds$class==test[,3])/n
# e)
modelo.cv=lda(test[,3]~test[,-3],CV=TRUE) ; table(test[,3],modelo.cv$class)
sum(loopred[1,1]+loopred[2,2])/n
odelo.cv=lda(test[,3]~test[,-3],CV=TRUE)
loopred=table(test[,3],modelo.cv$class)
sum(loopred[1,1]+loopred[2,2])/n
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X1=cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X2=cbind(x1,x2,y)
X=rbind(X1,X2)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X1=cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X2=cbind(x1,x2,y)
X=rbind(X1,X2)
modelo=qda(X[,3]~X[,-3])
plot(modelo)
modelo
modelo$scaling
modelo$scaling/w
ls(modelo)
modelo$lev
modelo$means
preds=predict(modelo, data.frame(test))
preds
)
n=100
y=0
x1=rnorm(n/2,1,1)
x2=rnorm(n/2,3,1)
X1=cbind(x1,x2,y)
y=1
x1=rnorm(n/2,2,1)
x2=rnorm(n/2,2,1)
X2=cbind(x1,x2,y)
X=rbind(X1,X2)
# b)
plot(X[,1],X[,2], col=c("red","blue")[X[,3]+1], pch=20)
# c)
X_1=X1[1:50,-3]
X_2=X2[1:50,-3]
S_1=var(X_1)
S_2=var(X_2)
S=1/98*(49*S_1+49*S_2)
S_inv=solve(S)
d=colMeans(X_1)-colMeans(X_2)
w=S_inv%*%as.matrix(d)
modelo=lda(X[,3]~X[,-3])
plot(modelo)
# vemos que tienen la misma dirección
modelo$scaling/w
# d)
n=100
y=0
x1_t=rnorm(n/2,1,1)
x2_t=rnorm(n/2,3,1)
X1_t=cbind(x1_t,x2_t,y)
y=1
x1_t=rnorm(n/2,2,1)
x2_t=rnorm(n/2,2,1)
X2_t=cbind(x1_t,x2_t,y)
test=rbind(X1_t,X2_t)
preds=predict(modelo, data.frame(test))
preds$class
# % de bien clasificados
sum(preds$class==test[,3])/n
# e)
modelo.cv=lda(test[,3]~test[,-3],CV=TRUE)
loopred=table(test[,3],modelo.cv$class)
sum(loopred[1,1]+loopred[2,2])/n
modelo.qda=qda(X[,3]~X[,-3])
preds=predict(modelo, data.frame(test))
modelo.qda=qda(X[,3]~X[,-3])
preds.qda=predict(modelo.qda, data.frame(test))
preds.qda$class
# % de bien clasificados
sum(preds.qda$class==test[,3])/n
modelo.cv.qda=lda(test[,3]~test[,-3],CV=TRUE)
loopred.qda=table(test[,3],modelo.cv.qda$class)
sum(loopred.qda[1,1]+loopred.qda[2,2])/n
# g)
modelo.logit=glm(X[,3]~X[,-3])
modelo.logit
modelo.logit=glm(X[,3]~X[,-3])
preds.glm=predict(modelo.logit, data.frame(test))
# % de bien clasificados
sum(preds.glm$class==test[,3])/n
preds.glm
sum(as.numeric(preds.glm>.5)==test[,3])/n
rm(list=ls(all=TRUE))
ls()
library(MASS)
#############################
#Primer Ejemplo##############
#############################
#Consumición anual en franco de 8 tipo de comida/bebida (variables) por 8 categorias socio-profesionales.
#Variables: 1 Pan común, 2 Otro tipo de pan, 3 Vino común, 4 Otro tipo de vino,
# 5 Papas, 6 Vegetales, 7 Uva, 8 Plato preparado
#Individus 1 Productor rural, 2 Asalariado rural, 3 Profesional independiente,
#4 Ejecutivo superior, 5 Ejecutivo medio, 6 Empleado, 7 Obrero, 8 Desocupado
X=t(matrix(c(167,1,163,23,41,8,6,6,162,2,141,12,40,12,4,15,119,6,69,56,39,5,
13,41,87,11,63,111,27,3,18,39,103,5,68,77,32,4,11,30,111,4,72,66,34,6,10,28,130,
3,76,52,43,7,7,16,138,7,117,74,53,8,12,20),nrow=8) )
colnames(X)=c("PC", "OP", "VC","OV","P","Veg","Uva", "Platos")
rownames(X)=c("PRodRu", "Asalrur","Prof","Ejsup","Ejmoy","Emp","Obr", "Des")
X
#Cuentas descriptivas a partir de X.
#Promedio por columnas
colMeans(X)
apply(X,2,mean)
#Varianza por columnas
apply(X,2,var)
#Desviaci?n estandar por columnas
s=apply(X,2,sd)
s
#Matriz de datos centrados y reducidos de X
Z= scale(X, center = TRUE, scale = TRUE)
Z
#Matriz varianza/covarianza
V=cov(X)
V
#Matriz correlaciones
R=cor(X)
R
#observar que cor(X)=cor(Z)
#Valores y vectores propios de R
valp=round(eigen(R)$values,3)
vecp=eigen(R)$vectors
#Para ver porcentaje de la variación explicada
cumsum(valp)/sum(valp)
#Matriz de vectores propios
A=vecp
#Veamos las coordenadas de los individuos sobre los nuevos ejes:
Y=Z%*%A
plot(Y[,1:2])
#Obs:
#1) La variabilidad es más grande según el eje 1
#2) Oposición entre "agricultores" y ejecutivos superiores
#3) el segundo eje es característico de los inactivos
#Ahora vamos hacer lo mismo con la función PCA del paquete FactomineR.
library(FactoMineR)
a=PCA(X)
#Los valores propios y porcentajes de variabilidad
#FactoMineR trabaja con la matriz de correlaciones
a$eig
#Las correlaciones entre Z_i y X_j.
a$var$coord
#La primer componente mide la repartición de la consumición entre alimentos
#básicos (PC,VC,Veg) y alimentos más refinados (OP, OV, Uva, Platos)
#La segunda tiene que ver más bien con la consumición de papas,
#consumición elevada para los inactivos
#Ver que me da bien lo mismo
vecp[,1]/a$var$coord[,1]
vecp[,2]/a$var$coord[,2]
#los vectores construidos son colineales!
#recordar que si la matriz es reducida/estandarizada
#(media cero/cada col tiene desvio 1) entonces
#cor(z_j,x_i)=cos(z_j,x_1)
#coordendas de los individuos sobre los ejes.
a$ind$coord
#Mapa de factores: correlación de los datos de las viejas variables con los dos primeros ejes.
#Mapa de individuos: proyección de los individuos sobre los dos primeros ejes.
#Con dimdesc(a) vemos las correlaciones de las nuevas variables con las
#viejas variables, escritas en orden de significatividad en cuanto a su coeficiente
#de correlación.
install.packages("FactoMineR")
rm(list=ls(all=TRUE))
ls()
library(MASS)
#############################
#Primer Ejemplo##############
#############################
#Consumición anual en franco de 8 tipo de comida/bebida (variables) por 8 categorias socio-profesionales.
#Variables: 1 Pan común, 2 Otro tipo de pan, 3 Vino común, 4 Otro tipo de vino,
# 5 Papas, 6 Vegetales, 7 Uva, 8 Plato preparado
#Individus 1 Productor rural, 2 Asalariado rural, 3 Profesional independiente,
#4 Ejecutivo superior, 5 Ejecutivo medio, 6 Empleado, 7 Obrero, 8 Desocupado
X=t(matrix(c(167,1,163,23,41,8,6,6,162,2,141,12,40,12,4,15,119,6,69,56,39,5,
13,41,87,11,63,111,27,3,18,39,103,5,68,77,32,4,11,30,111,4,72,66,34,6,10,28,130,
3,76,52,43,7,7,16,138,7,117,74,53,8,12,20),nrow=8) )
colnames(X)=c("PC", "OP", "VC","OV","P","Veg","Uva", "Platos")
rownames(X)=c("PRodRu", "Asalrur","Prof","Ejsup","Ejmoy","Emp","Obr", "Des")
X
#Cuentas descriptivas a partir de X.
#Promedio por columnas
colMeans(X)
apply(X,2,mean)
#Varianza por columnas
apply(X,2,var)
#Desviaci?n estandar por columnas
s=apply(X,2,sd)
s
#Matriz de datos centrados y reducidos de X
Z= scale(X, center = TRUE, scale = TRUE)
Z
#Matriz varianza/covarianza
V=cov(X)
V
#Matriz correlaciones
R=cor(X)
R
#observar que cor(X)=cor(Z)
#Valores y vectores propios de R
valp=round(eigen(R)$values,3)
vecp=eigen(R)$vectors
#Para ver porcentaje de la variación explicada
cumsum(valp)/sum(valp)
#Matriz de vectores propios
A=vecp
#Veamos las coordenadas de los individuos sobre los nuevos ejes:
Y=Z%*%A
plot(Y[,1:2])
#Obs:
#1) La variabilidad es más grande según el eje 1
#2) Oposición entre "agricultores" y ejecutivos superiores
#3) el segundo eje es característico de los inactivos
#Ahora vamos hacer lo mismo con la función PCA del paquete FactomineR.
library(FactoMineR)
a=PCA(X)
#Los valores propios y porcentajes de variabilidad
#FactoMineR trabaja con la matriz de correlaciones
a$eig
#Las correlaciones entre Z_i y X_j.
a$var$coord
#La primer componente mide la repartición de la consumición entre alimentos
#básicos (PC,VC,Veg) y alimentos más refinados (OP, OV, Uva, Platos)
#La segunda tiene que ver más bien con la consumición de papas,
#consumición elevada para los inactivos
#Ver que me da bien lo mismo
vecp[,1]/a$var$coord[,1]
vecp[,2]/a$var$coord[,2]
#los vectores construidos son colineales!
#recordar que si la matriz es reducida/estandarizada
#(media cero/cada col tiene desvio 1) entonces
#cor(z_j,x_i)=cos(z_j,x_1)
#coordendas de los individuos sobre los ejes.
a$ind$coord
#Mapa de factores: correlación de los datos de las viejas variables con los dos primeros ejes.
#Mapa de individuos: proyección de los individuos sobre los dos primeros ejes.
#Con dimdesc(a) vemos las correlaciones de las nuevas variables con las
#viejas variables, escritas en orden de significatividad en cuanto a su coeficiente
#de correlación.
